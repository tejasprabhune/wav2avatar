{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "from wav2avatar.streaming.hifigan.wav_ema_dataset import WavEMADataset\n",
    "from wav2avatar.streaming.hifigan.layers import PastFCEncoder, HiFiGANResidualBlock\n",
    "\n",
    "import typing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: (B, 512, T)\n",
    "# Output: (B, 512, ws), where ws is window size\n",
    "\n",
    "# E.g.: T = 615, ws = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_features(features: typing.List[torch.FloatTensor], window_size=50):\n",
    "    \"\"\"\n",
    "    Performs the autoregressive collating from the car_collate function.\n",
    "    \"\"\"\n",
    "    feats_collated = []\n",
    "    for feature in features:\n",
    "        collates = [feature[:, :, i:i+window_size] for i in range(0, feature.shape[2] // window_size * window_size, window_size)]\n",
    "        feature_collated = torch.cat(collates, dim=0)\n",
    "        feats_collated.append(feature_collated)\n",
    "    return torch.concatenate(feats_collated, dim=0)\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    wav_ema_dataset = WavEMADataset()\n",
    "#\n",
    "#    data_loader = torch.utils.data.DataLoader(dataset=wav_ema_dataset, batch_size=1, shuffle=True, collate_fn=car_collate)\n",
    "#\n",
    "#    print(next(iter(data_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def car_collate(batch):\n",
    "    \"\"\"\n",
    "    Collates a batch of audio_features and corresponding EMA features in the\n",
    "    following way:\n",
    "\n",
    "    For time interval [curr, curr + window_size], we use the audio features\n",
    "    and pseudolabeled EMA from the same time interval \n",
    "    [curr, curr + window_size]. As we are autoregressive w.r.t to the EMA \n",
    "    predictions, we use the EMA_ar features from the previous time interval \n",
    "    [curr - window_size, curr].\n",
    "\n",
    "    We then concatenate the audio features, EMA features, EMA_ar features\n",
    "    independently through all time steps, jumping by window_size.\n",
    "\n",
    "    Args:\n",
    "        batch: Batch of tuple - (audio_features, ema_features)\n",
    "\n",
    "    Returns:\n",
    "        audio_feats_collated: Collated audio features\n",
    "        ema_collated: Collated EMA features\n",
    "        ema_collated_ar: Collated EMA_ar features\n",
    "    \"\"\"\n",
    "    audio_feats, ema = zip(*batch)\n",
    "\n",
    "    audio_feats = list(audio_feats)\n",
    "    ema = list(ema)\n",
    "\n",
    "    for i in range(len(audio_feats)):\n",
    "        audio_feats[i] = audio_feats[i].detach().cpu()\n",
    "        ema[i] = ema[i].detach().cpu()\n",
    "\n",
    "    window_size = 50\n",
    "    audio_feats_collated = collate_features(audio_feats, window_size)\n",
    "\n",
    "    ema_collated = collate_features(ema, window_size).float()\n",
    "    ema_collated_ar = ema_collated[:len(ema_collated) - 1]\n",
    "    first_ema_batch = torch.zeros(1, 12, window_size)\n",
    "\n",
    "    ema_collated_ar = torch.concatenate([first_ema_batch, ema_collated_ar], dim=0).float()\n",
    "\n",
    "    return audio_feats_collated, ema_collated, ema_collated_ar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_model = PastFCEncoder(input_len=600, hidden_dim=256, output_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial x: torch.Size([1, 512, 615]), ar: torch.Size([1, 12, 50])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ar_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m615\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial x: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, ar: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mar\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m ar_feats \u001b[38;5;241m=\u001b[39m \u001b[43mar_model\u001b[49m(ar)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m615\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mar feats: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mar_feats\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, ar_feats], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ar_model' is not defined"
     ]
    }
   ],
   "source": [
    "ar = torch.randn(1, 12, 50)\n",
    "x = torch.randn(1, 512, 615)\n",
    "print(f\"Initial x: {x.shape}, ar: {ar.shape}\")\n",
    "\n",
    "ar_feats = ar_model(ar).unsqueeze(2).repeat(1, 1, 615)\n",
    "print(f\"ar feats: {ar_feats.shape}\")\n",
    "\n",
    "x = torch.cat([x, ar_feats], dim=1)\n",
    "print(f\"Concatenated x: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input conv: torch.Size([1, 256, 205])\n"
     ]
    }
   ],
   "source": [
    "input_conv = torch.nn.Conv1d(\n",
    "    512 + 128,\n",
    "    512 // 2,\n",
    "    3,\n",
    "    3,\n",
    "    padding=(3 - 1) // 2,\n",
    ")\n",
    "x = input_conv(x)\n",
    "print(f\"Input conv: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = []\n",
    "resblock_kernel_sizes=(3, 7, 11, 15)\n",
    "resblock_dilations=[(1, 3, 5), (3, 5, 7), (1, 3, 5), (1, 3, 5)]\n",
    "for j in range(len(resblock_kernel_sizes)):\n",
    "    blocks += [\n",
    "        HiFiGANResidualBlock(\n",
    "            kernel_size=resblock_kernel_sizes[j],\n",
    "            channels=512 // (2 ** (0 + 1)),\n",
    "            dilations=resblock_dilations[j],\n",
    "            bias=True,\n",
    "            use_additional_convs=True,\n",
    "            nonlinear_activation=\"LeakyReLU\",\n",
    "            nonlinear_activation_params={\"negative_slope\": 0.1},\n",
    "        )\n",
    "    ]\n",
    "\n",
    "cs = 0.0\n",
    "for block in blocks:\n",
    "    cs += block(x)\n",
    "\n",
    "x = cs / len(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_conv = torch.nn.Sequential(\n",
    "    # NOTE(kan-bayashi): follow official implementation but why\n",
    "    #   using different slope parameter here? (0.1 vs. 0.01)\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Conv1d(\n",
    "        512 // (2 ** (1)),\n",
    "        1,\n",
    "        3,\n",
    "        1,\n",
    "        padding=(3 - 1) // 2,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 205])\n"
     ]
    }
   ],
   "source": [
    "x = output_conv(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 2])\n",
      "torch.Size([1, 256])\n",
      "torch.Size([1, 128, 615])\n"
     ]
    }
   ],
   "source": [
    "# Input AR: (B, 12, 50)\n",
    "# Output feat: (B, 128)\n",
    "\n",
    "ar_conv = torch.nn.Sequential(\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Conv1d(\n",
    "        12,\n",
    "        128,\n",
    "        3,\n",
    "        1,\n",
    "        padding=1\n",
    "    ),\n",
    "    torch.nn.Conv1d(\n",
    "        128,\n",
    "        128,\n",
    "        3,\n",
    "        5,\n",
    "        padding=1\n",
    "    ),\n",
    "    torch.nn.Conv1d(\n",
    "        128,\n",
    "        128,\n",
    "        3,\n",
    "        5,\n",
    "        padding=1\n",
    "    )\n",
    ")\n",
    "ar_linear = torch.nn.Linear(256, 128)\n",
    "\n",
    "ar = torch.randn(1, 12, 50)\n",
    "ar = ar_conv(ar)\n",
    "print(ar.shape)\n",
    "ar = ar.reshape(ar.shape[0], -1)\n",
    "print(ar.shape)\n",
    "ar = ar_linear(ar)\n",
    "ar = ar.unsqueeze(2).repeat(1, 1, 615)\n",
    "print(ar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (128x10 and 50x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m ar \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      4\u001b[0m ar \u001b[38;5;241m=\u001b[39m ar_conv(ar)\n\u001b[0;32m----> 6\u001b[0m ar \u001b[38;5;241m=\u001b[39m \u001b[43mar_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(ar\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/w2a/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/w2a/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/w2a/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x10 and 50x128)"
     ]
    }
   ],
   "source": [
    "ar_linear = torch.nn.Linear(50, 128)\n",
    "\n",
    "ar = torch.randn(1, 12, 50)\n",
    "ar = ar_conv(ar)\n",
    "\n",
    "ar = ar_linear(ar)\n",
    "\n",
    "print(ar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6400"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128 * 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w2a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
